---
knit: (function(input_file, encoding) {
  out_dir <- 'docs/R';
  rmarkdown::render(input_file,
    encoding=encoding,
    output_file='../multinomial-logistic-regression.md') })
output:
  html_document:
    highlight: tango
editor_options:
  chunk_output_type: console
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, fig.path="../img/multinomial-logistic-regression/",
                      message = FALSE,
                      warning = FALSE,
                      fig.width=6, fig.height=3,
                      fig.show='hold', fig.align='center')
options(width=100)
```

# Multinomial Logistic Regression in `R`

For this guide we will assess what factors influence an employee's choice on one of three health insurance plans offered by a company.  The [data set](https://github.com/tylerbg/DLC_stat_resources/tree/master/docs/R/dat/health_insurance.csv), which is a reduced version of the data set provided in the [Handbook of Regression Modeling in People Analytics](https://peopleanalytics-regression-book.org/index.html), has the following 4 factors:

* *plan* - the health insurance plan chosen (plan A, B, or C).
* *age* - the person's age when they chose their healthcare plan.
* *gender* - the gender of the person (Male or Female).
* *absent* - the number of absentee days from work the year prior to choosing their health insurance plan.

There are 50 observations evenly split between Male and Female employees.

To begin, we will load in the data using `read.csv()`, then take a quick look at the first few rows with `head()` and get a summary of the data set with `summary()`.

```{r load-data}
health <- read.csv("../../dat/health_insurance.csv")

head(health)
summary(health)
```

In the output from `summary()` we can see that the *gender* and *plan* variables were read as character variables, however we would want them to be factor where there are 2 levels in *gender*, Male and Female, and 3 levels in *plan*, A, B, and C. We can easily change these variables to factors using `factor()` to reassign each variable. We can then use `str()` to see the structure of the data, which will allow us to see the variable types and some other basic information on those variables.

```{r assign-factors}
health$gender <- factor(health$gender)
health$plan <- factor(health$plan)

str(health)
```

We see that *gender* and *plan* are now factors with 2 and 3 levels, respectively. We can also note that the *age* and *absent* variables were set as integers rather than numeric, which is fine for our purposes here.

Now we will fit our multinomial model where we have the health insurance plans, *plan*, as our dependent variable with *age*, *gender*, and *absent* as predictors. `R` does not have a base multinomial regression function so we will need to import a library to fit our model. Two popular packages for multinomial logistic regression are `nnet` and `VGAM`. In this example we will use `VGAM` which is more friendly with some of the functions we used to assess model results in our other guides.

After installing and loading the `VGAM` package we will use `vglm()`, a function to fit multiple types of Generalized Linear Models. In `vglm()` we put the model formula in the same format as other models where the dependent variable is on the left and predictor variables on the right of a `~`, with `.` as a shortcut to include all variables not included on the left side of the equation. We then assign our data set to `data` and assign `multinomial` to `family` to sepecify we aim to fit a multinomial regression model.

In addition, we will want to fit a null model (one withour our predictor variables and only an intercept) that we can use to test the fit of our model. We could either write a null model in `vglm()` or we can use `update()`, where we include the object that is our original model followed by the desired model formula, which for a null model we put a `1` on the right side of the `~` to indicate we only want an intercept. After both models are fit we will use `lrtest()`, a function provided with `VGAM`, to perform a likelihood ratio test to compare the two models.

```{r fit-mlr}
library(VGAM)

health_mlr <- vglm(plan ~ .,
                   data = health,
                   family = multinomial)

health_mlr0 <- update(health_mlr, . ~ 1)

lrtest(health_mlr0, health_mlr)
```

From the results of the likelihood ratio test we can see that the log-likelihood (<code class="out">LogLik</code>) is higher in the model that includes our variables (second model in the output) compared to the null model. Importantly, this difference is significant with a very low p-value (<code class="out">Pr(>Chisq)</code> = <code class="out">1.228e-08</code>). We then have evidence that our model explains some of the variance compared to the null model and follow-up with the `summary()` function to assess some of the model's results.

```{r mlr-summary}
summary(health_mlr)
```

In the output we should first note the bottom line that says <code class="out">Reference group is level  3  of the response</code>, which corresponds to plan `C`. This line is telling us that the other levels, plans `A` (level 1) and `B` (level 2), are being compared to plan `C` in the <code class="out">Coefficients</code> table. In that table we see that two coefficients are statistically significant, <code class="out">age:1</code> and <code class="out">genderMale:2</code>. As both have negative coefficients we can interpret these results as:

* As *age* increases the log-likelood of an employee choosing plan `A` over plan `C` decreases.
* `Male` employees are less likely to choose plan `B` over plan `C` compared to `Female` employees.

We can look at plots of the data to see these differences more clearly.

```{r plots, echo = FALSE, fig.width = 7}
library(tidyverse)

p1 <- ggplot(health, aes(x = plan, y = age)) +
  stat_summary(geom = "pointrange",
               fun.data = mean_sdl,
               fun.args = list(mult = 1)) +
  stat_summary(geom = "errorbar",
               fun.data = mean_sdl,
               fun.args = list(mult = 1),
               width = 0.2) +
  geom_jitter(width = 0.1, height = 0,
              alpha = 0.2) +
  labs(x = "Plan",
       y = "Age",
       title = "Employees who pick one of three health\ninsurance plans based on their age") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

p2 <- ggplot(health, aes(x = plan, fill = gender, color = gender)) +
  geom_bar(position = "dodge", fill = "white") +
  geom_bar(position = "dodge", alpha= 0.5) +
  labs(x = "Plan",
       y = "Count",
       title = "Number of employees who pick one of three health\ninsurance plans separated by gender") +
  theme_bw() +
  scale_fill_discrete(name = "Gender") +
  scale_color_discrete(name = "Gender") +
  theme(plot.title = element_text(hjust = 0.5))

library(gridExtra)

grid.arrange(p1, p2, nrow = 1)
```

In the plot to the left we see how employees choose each health insurance plan by their age, where the black points represent the means and the bars the standard deviations for each *plan* and each grey point is an individual employee. It is pretty clear from the plot and with statistical evidence from our multinomial model that older employees favor plan `C` over plan `A`, while plans `B` and `C` are relatively similar in their age distributions. In the right bar plot, we can also clearly see, with statistical evidence, that `Male` employees tend to choose plan `C` over plan `B` compared to `Female` employees.

What about our other variable, *absent*, which did not have a significant coefficient? We can also graph it against *plan* to see if our statsitical results hold against observational scrutiny.

```{r absent-plot, echo = FALSE, fig.width = 4}
ggplot(health, aes(x = plan, y = absent)) +
  stat_summary(geom = "pointrange",
               fun.data = mean_sdl,
               fun.args = list(mult = 1)) +
  stat_summary(geom = "errorbar",
               fun.data = mean_sdl,
               fun.args = list(mult = 1),
               width = 0.2) +
  geom_jitter(width = 0.1, height = 0,
              alpha = 0.2) +
  labs(x = "Plan",
       y = "Number of days absent",
       title = "Employees who pick one of three health insurance plans based on the\nnumber of days absent in the past year") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
```

Again, it appears that the plot of our variables against one another agree with our statistical results, where the number of days absent does not significantly influence the choice of health insurance plan.

We should note that so far we have only compared two of our *plan* levels, `A` and `B`, against `C`, but we have not compared plans `A` and `B` for any of the three predictor variables. Since our levels are unordered in multinomial regression we likely want to compare each level of our response variable against all other levels. To do so, we can change the contrasts, which is a linear combination of parameters (that add to 0) which allow different comparisons of treatments.

To change our contrasts to compare plans `A` and `B` we will make a vector of parameters for each variable, then use the `ghlt()` function from the `multcomp` package to print out test results on those comparisons.

First, to make the vector of parameters we with wrap `c()`, which lists each parameter separated by a comma, into `matrix()` with `1` to indicate we want one 1 row. The parameters we include are going to be based on which levels we want to compare (`A` vs. `B`), the predictor variable to compare on, and where they are 'located.' If we look back at the coefficients table from the summary of our multinomial regression model we can see a list of 8 coefficients (the intercepts are included). So, to compare *age* between plans `A` and `B` we will need to change the 3rd and 4th contrast parameters, which we will make `-1` and `1`, respectively, so that their sum = 0. All of the other parameters we will leave at 0, so our final vectore to compare plans `A` and `B` by *age* is: `c(0, 0, -1, 1, 0, 0, 0, 0)`.

After loading the `multcomp` package, we will then make vectors to compare plans `A` and `B` on the *gender* and *absent* variables as well, then assign it to `linfct` in the `ghlt()` function which also wraps our original model. We will also wrap `ghlt()` in `summary()` to print out the results of each test.

```{r contrasts}
library(multcomp)

con_age <- matrix(c(0, 0, -1, 1, 0, 0, 0, 0), 1)
con_gender <- matrix(c(0, 0, 0, 0, -1, 1, 0, 0), 1)
con_absent <- matrix(c(0, 0, 0, 0, 0, 0, -1, 1), 1)

summary(glht(health_mlr, linfct = con_age))
summary(glht(health_mlr, linfct = con_gender))
summary(glht(health_mlr, linfct = con_absent))
```

From these results we can see that only the first two comparisons, for *age* and *gender*, are statitically significant and therefore can condlue:

* Younger employees have a higher log-likelihood of choosing plan `A` over plan `B`.
* `Male` employees are more likely to choose plan `A` over plan `B`.
* The number of days absent does not affect how likely an employee will choose between plans `A` and `B`.