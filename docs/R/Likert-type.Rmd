---
knit: (function(input_file, encoding) {
  out_dir <- 'docs/R';
  rmarkdown::render(input_file,
    encoding=encoding,
    output_file='Likert-type.md') })
output: github_document
editor_options:
  chunk_output_type: console
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, fig.path="img/Likert-type/",
                      fig.width=6, fig.height=4,
                      fig.show='hold', fig.align='center'
                      )
options(width=100)
```

# Likert-type scale analysis

&emsp;For this tutorial we will be using a modified data set that contains Likert responses collected from faculty at the Open University of Catalonia on their opinions for using Wikipedia as a teaching resource.  The data set was originally provided by the [UCI Maching Learning Repository](https://archive.ics.uci.edu/ml/datasets/wiki4he).

```{r load_wiki_data}
wiki.dat <- read.csv("dat/wiki4HE_rev.csv", stringsAsFactors = TRUE)
str(wiki.dat)
```

### Two group comparisons 

&emsp;Let us say that we are interested if two groups respond differently to a signle Likert-item.  In this case, we are interested if males and females generally have different views on the reliability of Wikipedia articles as a resource.  Viewing the results of the survey, as in the barplot below, suggests that male respondents view Wikipedia articles as more reliable compared to their female counterparts overall, with 44% of males selecing either "Agree" or "Strongly agree" compared to 29% of females.

```{r Qu1_GENDER_plot, echo = FALSE, results = FALSE, message = FALSE, fig.height = 2}
library(tidyverse)
library(likert)

Qu1.items <- wiki.dat[, 4, drop = FALSE]
Qu1.items[, 1] <- factor(Qu1.items$Qu1)
names(Qu1.items)[names(Qu1.items) == "Qu1"] <- "Quality 1: Articles in Wikipedia are reliable"
likert.res <- likert(items = Qu1.items, grouping = wiki.dat$GENDER)
plot(likert.res,
     low.color = "goldenrod",
     high.color = "steelblue",)
```

&emsp;To statistically test whether these differences between the genders exist, we must decide on the test to use.  Because the responses are ordinal in nature, where there are a small set number of possible responses that are ordered but do not have equal separation between their levels, parametric tests that rely on assumptions of approximately normal distributions such as the t-test are not appropriate.  Instead, non-parametric tests that do not make assumptions on how the data is distributed should be applied.  In the case of comparing two groups, males versus females in this example, the Mann-Whitney U test can be applied.

&emsp;The Mann-Whitney U test is commonly referred to the non-parametric equivalent to the parametric t-test, using the U-statistic over the t-statistic.  Where the t-test tests the hypothesis that the means are equal, the Mann-Whitney U test instead test whether the medians of two groups come from the same distribution.

#### Hypotheses

&emsp;Of course, we want to form our hypothesis before we look at our data, which we can write as:

&emsp;<b><i>H</i><sub>0</sub>:<i></b> The probability that a randomly drawn male or female respondent is larger than a randomly drawn respondent from the respective opposite group = 50%</i>  
&emsp;<b><i>H</i><sub>A</sub>:<i></b> The probability that a randomly drawn male or female respondent is larger than a randomly drawn respondent from the respective opposite group &ne; 50%</i>

&emsp;While wordy, what we are saying in our null hypothesis is that both groups respond similarly while our alternative hypothesis is that one group (either male or female) answers differently (higher or lower) compared to the other group.

&emsp;To perform a Mann-Whitney U test in R we can use the `wilcox.test()` function, which defaults to using the Mann-Whitney test when we have two unpaired samples (also known as the Wilcoxon two-sample test).  For this example we will use a statistical threshold of &alpha; = 0.05.

```{r mann_whit_test}
wilcox.test(Qu1 ~ GENDER, data = wiki.dat)
```

&emsp;The results of the Mann-Whitney U test, a p-value less than 0.05, indicate that there is a statistically significant difference between how males and females respond to how they view the reliability of WIkipedia articles.

&emsp;*Note: Alternatively, we could use the Kruskal-Wallis test, which can also be applied when we want to compare more than two groups as follows.*

### Multiple (>2) group comparisons

&emsp;When we want to compare more than two groups, such as if we want to compare the responses to quality item 1 among the different 'domains', the Kruskal-Wallis test can be applied.  From the following plot of the responses by domain, we might think that respondents for the Sciences and the Engineering & Architecture domains are more positive in their thinking of Wikipedia's reliability than say for Law & Politics or Health Sciences.

```{r Qu1_DOMAIN_plot, echo = FALSE, fig.height = 3.5}
likert.res2 <- likert(items = Qu1.items, grouping = wiki.dat$DOMAIN)
plot(likert.res2,
     plot.title = "Articles in Wikipedia are reliable",
     low.color = "goldenrod",
     high.color = "steelblue",
     group.order = c("Arts & Humanities",
                     "Engineering & Architecture",
                     "Health Sciences",
                     "Law & Politics",
                     "Sciences",
                     "Other"))
```

&emsp;Similarly to the two group comparisons as above, we should use a non-parametric test to determine if there are statistical differences among how the domains respond to Wikipedia article reliability.  Just like we used a non-parametric equivalent to the t-test to compare two groups, we can use a non-parametric equivalent to the ANOVA to test multiple groups such as the Kruskall-Wallis test.

#### Hypotheses

&emsp;<b><i>H</i><sub>0</sub>:<i></b> The medians of all domains are equal</i>  
&emsp;<b><i>H</i><sub>A</sub>:<i></b> The median of at least one domain is not equal to to at least one other domain</i>

&emsp;We can run a simple Kruskall-Wallis test in base R using the `kruskal.test` command as follows.

```{r kruskal_test}
kruskal.test(Qu1 ~ DOMAIN, data = wiki.dat)
```

&emsp;While the Kruskal-Wallis test indicates that there is a significantly different mean among the groups (p-value is less than 0.05), it does not indicate which of those means are statistically different from the others.  We need to follow up this test with a pairwise test that can compare each of the means against one another.

&emsp;The Dunn's test is the most common post-hoc test for the Kruskal-Wallis test, however a function to perform the test is not included in base R.  We will need to install and load the `FSA` library which contains the `dunnTest()` function to perform the post-hoc analysis.

```{r dunn_test1}
library(FSA)

dunnTest(Qu1 ~ DOMAIN, data = wiki.dat, method = "bonferroni")
```

&emsp;Since we have 6 different groups that we are comparaing against one another (pairwise), our output consists of 15 pairwise comparisons.  As our number of groups that we are comparing increases (k), the number of pairwise comparisons increases by k(k-1)/2.  Therefore, as more groups are included in the analysis the number of comparisons, and thus adjustments for multiple comparisons must be performed.

```{r dunn_test2}
dunn.res <- dunnTest(Qu1 ~ DOMAIN, data = wiki.dat, method = "bonferroni")
dunn.res$res$sig <- ifelse(dunn.res$res$P.adj <= 0.05, "*", " ")

dunn.res
```

&emsp;We can see that indeed both the Engineering & Architecture and the Sciences domains rate the reliability of Wikipedia significantly higher overall than Law & Politics.  WHile only the pairwise comparison between Sciences and Health Sciences is statistically significant, the comparison between Health Sciences and Engineering & Architecture is very close to the threshold.  While we have another pairwise comparison that is statistically significant, between Law & Politics and Other, because the Other group could be quite broad and nonspecific, the results from this comparison may not be much use for us, and we could consider taking "Other" out of the domain analysis entirely.

&emsp;*Note that we used the Bonferroni correction here which is a very conservative approach to handle multiple test errors.  The `dunnTest()` function can perform other types of multiple correction for which more information can be found by submitting `?dunnTest` into the R console.*

### Statistical analysis with a continuous independent variable

&emsp;What if we hypothesize there is a relationship in how respondents view Wikipedia reliability by their age?  If we look at the density-point plot below it might be difficult to make any conclusions.  Thankfully, we can use a statistical test ot help answer our question!

```{r olr_plot, echo = FALSE}
ggplot(wiki.dat, aes(x = AGE, y = Qu1, color = "steelblue", fill = "steelblue")) +
  geom_point(alpha = 0.1) +
  scale_color_manual(values = rep("steelblue", 5)) +
  theme_bw() +
  theme(legend.position = "none")
```

&emsp;When we have a continuous (or approximately-continuous) variable that we are interested in, such as the age of the respondent, the group-wise comparison tests above are no longer applicable.  Instead, we can use regression techqniques.  However, because our data is ordinal, where the differences between each level are note equal, linear regression is not appropriate.  Instead, we can use the Ordinal Linear Regression (OLR) to statistical determine if there is some relationship between respondent age and how they view Wikipedia reliability.

&emsp;While base R does not include a function to perform OLR, the commonly used `MASS` package has the easy to use `polr()` function that can fit the OLR model for us.

```{r olr_fit}
library(MASS)

olr.fit <- polr(factor(Qu1, ordered = TRUE) ~ AGE, data = wiki.dat, Hess = TRUE)
summary(olr.fit)
```

&emsp;While the `polr()` function calculates our test statistic (t-value) for the coefficients of the model, it does not automatically calculate p-values for us.  Instead, we can determine the respective p-values using the following commands, most notably the `pnorm()` function which calculates the p-value from the test statistic on the t-distribution.

```{r olr_results}
results.table <- coef(summary(olr.fit))
p.values <- pnorm(abs(results.table[, "t value"]), lower.tail = FALSE) * 2
(results.table <- cbind(results.table, "p-value" = p.values))
```

&emsp;In the above code, we first create a table of the coefficients (like we see from the `summary()` command).  Then, we calculate the p-values using `pnorm()` and append the results to our table with `cbind()`.

&emsp;From the results of the OLR model, the coefficient of `AGE` is statistically significant (less than 0.05) and we can conclude that there is a relationship between the age of the respondents and how they view the reliability of Wikipedia articles.  Further, because the value of the `AGE` coefficient is negative we can conclude that that relationship is negative, meaning that repondents of older ages tend to view Wikipedia reliability more negatively than those of younger ages.




